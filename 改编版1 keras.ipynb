{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import traceback\n",
    "pd.set_option('display.max_rows', 20,'max_info_columns', 9999,'display.max_columns', 9999)\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing,metrics\n",
    "import datetime as dt\n",
    "import gc\n",
    "from sklearn import svm,linear_model\n",
    "from collections import Counter\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder,Imputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,VotingClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "import jieba #导入结巴分词\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.cross_validation import  cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import xlrd\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/home/tsl-yu/文档/云移杯/YNU.EDU2018-ScenicWord/train_first.csv')\n",
    "\n",
    "test=pd.read_csv('/home/tsl-yu/文档/云移杯/YNU.EDU2018-ScenicWord/predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('/home/tsl-yu/文档/云移杯/YNU.EDU2018-ScenicWord_submite_sample.csv',names='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(train['Score']-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('Score',axis=1,inplace=True)\n",
    "train=pd.concat([train,test],axis=0,ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计詞频 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  创建停用詞列表，以及句子去掉停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r',encoding='gb2312').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对句子去除停用词\n",
    "def movestopwords(sentence,stopwords):\n",
    "    \n",
    "    outstr = ''\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t'and'\\n':\n",
    "                outstr += word\n",
    "                # outstr += \" \"\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwl=stopwordslist('/home/tsl-yu/文档/云移杯/Chinese-master/dict/四川大学机器智能实验室停用词库.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwl.extend([' ','，','。','的','了',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list(df):\n",
    "    \n",
    "    w = []\n",
    "    for i in df:\n",
    "        w.extend(i)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.546 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "cw = lambda x: list(jieba.cut(x,cut_all=True)) #定义分词函数\n",
    "train['Discuss']=[movestopwords(a,stopwl) for a in train['Discuss']]\n",
    "train['words'] = train['Discuss'].apply(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=word_list(train['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary=build_dataset(word,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(df,dny):\n",
    "    out=[]\n",
    "    for x in df:\n",
    "        y=[]\n",
    "        for k in x:\n",
    "            if k in dny:\n",
    "                y.append(dny[k])\n",
    "            else :\n",
    "                y.append(0)\n",
    "        out.append(y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sent']=get_sent(train['words'],dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad sequences (samples x time)\")\n",
    "train['sent'] = list(sequence.pad_sequences(train['sent'], maxlen=maxlen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(train['sent'][:100000]))[::2] #训练集\n",
    "y_ = np.array(y)[::2]\n",
    "\n",
    "xt = np.array(list(train['sent'][:100000]))[1::2] #测试集\n",
    "yt = np.array(y)[1::2]\n",
    "\n",
    "x_sub = np.array(list(train['sent'][100000:])) #提交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 0.9137 - acc: 0.6023 - val_loss: 0.8892 - val_acc: 0.6090\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.8522 - acc: 0.6243 - val_loss: 0.8850 - val_acc: 0.6121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7913573e80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary), 256))\n",
    "model.add(LSTM(256)) # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x, y_, batch_size=32, nb_epoch=2,validation_data=(xt,yt)) #训练时间为若干个小时\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.predict_classes(xt,batch_size=32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =np.argmax(yt,axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=metrics.mean_squared_error(y_test,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5985586707209041"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Discuss']=model.predict_classes(x_sub,batch_size=32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('sub_first.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
